# Enterprise-Grade Features - Part 2
## Real-Time Monitoring, Load Testing & PWA

**Version:** 3.0 - Enterprise Edition (Part 2)  
**Created:** October 31, 2025  
**Continues from:** `OPTIMIZATION_ENTERPRISE_FEATURES.md`

---

## üìä FEATURE 4: Real-Time Performance Monitoring

**File:** `monitoring/real_time_monitor.py` (NEW)

```python
"""
Real-Time System Monitoring
Tracks performance metrics with alerting
"""

import psutil
import threading
import time
import statistics
from collections import deque
from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging

logger = logging.getLogger('monitoring')

class RealTimeMonitor:
    """
    Real-time system performance monitor
    
    Features:
    - CPU, memory, disk monitoring
    - Response time tracking
    - Error rate calculation
    - Automatic alerting
    - Dashboard data generation
    - Historical metrics
    """
    
    def __init__(self):
        """Initialize monitoring system"""
        # Metrics history (last 1000 data points = ~1.5 hours at 5s interval)
        self.metrics_history = deque(maxlen=1000)
        
        # Alert thresholds
        self.alert_thresholds = {
            'cpu_percent': 80,
            'memory_percent': 85,
            'disk_percent': 90,
            'response_time_ms': 2000,
            'error_rate_percent': 5,
            'database_connections': 8  # 80% of pool size
        }
        
        # Monitoring state
        self._monitoring = False
        self._monitor_thread = None
        
        # Request tracking
        self._request_times = deque(maxlen=1000)
        self._request_errors = deque(maxlen=1000)
        
        # Alert cooldown (prevent spam)
        self._last_alert_time = {}
        self._alert_cooldown = 300  # 5 minutes
    
    def start_monitoring(self):
        """Start background monitoring thread"""
        if not self._monitoring:
            self._monitoring = True
            self._monitor_thread = threading.Thread(
                target=self._monitor_loop,
                daemon=True,
                name='RealTimeMonitor'
            )
            self._monitor_thread.start()
            logger.info("‚úÖ Real-time monitoring started")
    
    def stop_monitoring(self):
        """Stop monitoring thread"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=10)
        logger.info("üõë Real-time monitoring stopped")
    
    def record_request(self, response_time_ms: float, error: bool = False):
        """
        Record request metrics
        
        Args:
            response_time_ms: Response time in milliseconds
            error: Whether request resulted in error
        """
        self._request_times.append(response_time_ms)
        self._request_errors.append(1 if error else 0)
    
    def _monitor_loop(self):
        """Main monitoring loop (runs in background thread)"""
        logger.info("üîÑ Monitor loop started")
        
        while self._monitoring:
            try:
                # Collect metrics
                metrics = self._collect_metrics()
                
                # Store in history
                self.metrics_history.append(metrics)
                
                # Check for alerts
                self._check_alerts(metrics)
                
                # Sleep for 5 seconds
                time.sleep(5)
                
            except Exception as e:
                logger.error(f"Error in monitor loop: {e}")
                time.sleep(5)
    
    def _collect_metrics(self) -> Dict[str, Any]:
        """Collect current system metrics"""
        # System metrics
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # Network I/O
        net_io = psutil.net_io_counters()
        
        # Database stats (if available)
        db_stats = {}
        try:
            from database.connection_pool import db_manager
            db_stats = db_manager.get_stats()
        except:
            pass
        
        # Cache stats (if available)
        cache_stats = {}
        try:
            from caching.multi_level_cache import cache_manager
            cache_stats = cache_manager.get_stats()
        except:
            pass
        
        # Request metrics
        recent_request_times = list(self._request_times)[-100:]  # Last 100 requests
        recent_errors = list(self._request_errors)[-100:]
        
        avg_response_time = statistics.mean(recent_request_times) if recent_request_times else 0
        p95_response_time = (
            statistics.quantiles(recent_request_times, n=20)[18] 
            if len(recent_request_times) > 20 else 0
        )
        error_rate = (sum(recent_errors) / len(recent_errors) * 100) if recent_errors else 0
        
        return {
            'timestamp': datetime.now(),
            'system': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_used_gb': memory.used / (1024**3),
                'memory_available_gb': memory.available / (1024**3),
                'disk_percent': disk.percent,
                'disk_used_gb': disk.used / (1024**3),
                'disk_free_gb': disk.free / (1024**3)
            },
            'network': {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv,
                'packets_sent': net_io.packets_sent,
                'packets_recv': net_io.packets_recv
            },
            'requests': {
                'avg_response_time_ms': avg_response_time,
                'p95_response_time_ms': p95_response_time,
                'error_rate_percent': error_rate,
                'total_requests': len(self._request_times)
            },
            'database': db_stats,
            'cache': cache_stats
        }
    
    def _check_alerts(self, metrics: Dict[str, Any]):
        """Check metrics against thresholds and trigger alerts"""
        alerts = []
        
        # CPU check
        cpu = metrics['system']['cpu_percent']
        if cpu > self.alert_thresholds['cpu_percent']:
            alerts.append({
                'type': 'cpu',
                'severity': 'warning',
                'message': f"High CPU usage: {cpu:.1f}%",
                'value': cpu,
                'threshold': self.alert_thresholds['cpu_percent']
            })
        
        # Memory check
        memory = metrics['system']['memory_percent']
        if memory > self.alert_thresholds['memory_percent']:
            alerts.append({
                'type': 'memory',
                'severity': 'warning',
                'message': f"High memory usage: {memory:.1f}%",
                'value': memory,
                'threshold': self.alert_thresholds['memory_percent']
            })
        
        # Disk check
        disk = metrics['system']['disk_percent']
        if disk > self.alert_thresholds['disk_percent']:
            alerts.append({
                'type': 'disk',
                'severity': 'critical',
                'message': f"High disk usage: {disk:.1f}%",
                'value': disk,
                'threshold': self.alert_thresholds['disk_percent']
            })
        
        # Response time check
        p95_time = metrics['requests']['p95_response_time_ms']
        if p95_time > self.alert_thresholds['response_time_ms']:
            alerts.append({
                'type': 'response_time',
                'severity': 'warning',
                'message': f"Slow response times: P95 = {p95_time:.0f}ms",
                'value': p95_time,
                'threshold': self.alert_thresholds['response_time_ms']
            })
        
        # Error rate check
        error_rate = metrics['requests']['error_rate_percent']
        if error_rate > self.alert_thresholds['error_rate_percent']:
            alerts.append({
                'type': 'error_rate',
                'severity': 'critical',
                'message': f"High error rate: {error_rate:.1f}%",
                'value': error_rate,
                'threshold': self.alert_thresholds['error_rate_percent']
            })
        
        # Database connections check
        if 'active_connections' in metrics.get('database', {}):
            active_conns = metrics['database']['active_connections']
            if active_conns > self.alert_thresholds['database_connections']:
                alerts.append({
                    'type': 'database',
                    'severity': 'warning',
                    'message': f"High DB connections: {active_conns}",
                    'value': active_conns,
                    'threshold': self.alert_thresholds['database_connections']
                })
        
        # Send alerts (with cooldown)
        for alert in alerts:
            self._send_alert(alert)
    
    def _send_alert(self, alert: Dict[str, Any]):
        """
        Send alert notification
        
        Implements cooldown to prevent spam
        """
        alert_key = f"{alert['type']}:{alert['severity']}"
        now = time.time()
        
        # Check cooldown
        if alert_key in self._last_alert_time:
            if now - self._last_alert_time[alert_key] < self._alert_cooldown:
                return  # Still in cooldown
        
        # Update last alert time
        self._last_alert_time[alert_key] = now
        
        # Log alert
        log_func = logger.warning if alert['severity'] == 'warning' else logger.critical
        log_func(f"üö® ALERT: {alert['message']}")
        
        # TODO: Implement actual alerting (email, Slack, etc.)
        # For now, just log
    
    def get_dashboard_data(self, time_range_minutes: int = 5) -> Dict[str, Any]:
        """
        Get metrics for dashboard display
        
        Args:
            time_range_minutes: How many minutes of history to include
        
        Returns:
            Dict with current metrics, averages, and timeline
        """
        if not self.metrics_history:
            return {
                'status': 'no_data',
                'message': 'Monitoring data not yet available'
            }
        
        # Get recent metrics
        cutoff_time = datetime.now() - timedelta(minutes=time_range_minutes)
        recent_metrics = [
            m for m in self.metrics_history 
            if m['timestamp'] > cutoff_time
        ]
        
        if not recent_metrics:
            recent_metrics = list(self.metrics_history)[-60:]  # Last 60 points
        
        # Current metrics (most recent)
        current = recent_metrics[-1] if recent_metrics else {}
        
        # Calculate averages
        averages = {}
        if recent_metrics:
            averages = {
                'cpu_percent': statistics.mean(m['system']['cpu_percent'] for m in recent_metrics),
                'memory_percent': statistics.mean(m['system']['memory_percent'] for m in recent_metrics),
                'disk_percent': statistics.mean(m['system']['disk_percent'] for m in recent_metrics),
                'avg_response_time_ms': statistics.mean(m['requests']['avg_response_time_ms'] for m in recent_metrics),
                'error_rate_percent': statistics.mean(m['requests']['error_rate_percent'] for m in recent_metrics)
            }
        
        # Timeline data (for charts)
        timeline = []
        for m in recent_metrics[::5]:  # Sample every 5th point
            timeline.append({
                'time': m['timestamp'].strftime('%H:%M:%S'),
                'cpu': round(m['system']['cpu_percent'], 1),
                'memory': round(m['system']['memory_percent'], 1),
                'response_time': round(m['requests']['avg_response_time_ms'], 0)
            })
        
        return {
            'status': 'ok',
            'current': current,
            'averages': averages,
            'timeline': timeline,
            'thresholds': self.alert_thresholds,
            'data_points': len(recent_metrics),
            'time_range_minutes': time_range_minutes
        }
    
    def get_health_status(self) -> Dict[str, Any]:
        """
        Get overall system health status
        
        Returns:
            Dict with health status and issues
        """
        if not self.metrics_history:
            return {'status': 'unknown', 'message': 'No monitoring data'}
        
        current = self.metrics_history[-1]
        issues = []
        
        # Check each metric
        if current['system']['cpu_percent'] > self.alert_thresholds['cpu_percent']:
            issues.append('High CPU usage')
        
        if current['system']['memory_percent'] > self.alert_thresholds['memory_percent']:
            issues.append('High memory usage')
        
        if current['system']['disk_percent'] > self.alert_thresholds['disk_percent']:
            issues.append('Low disk space')
        
        if current['requests']['p95_response_time_ms'] > self.alert_thresholds['response_time_ms']:
            issues.append('Slow response times')
        
        if current['requests']['error_rate_percent'] > self.alert_thresholds['error_rate_percent']:
            issues.append('High error rate')
        
        # Determine overall status
        if not issues:
            status = 'healthy'
        elif len(issues) == 1:
            status = 'degraded'
        else:
            status = 'unhealthy'
        
        return {
            'status': status,
            'issues': issues,
            'metrics': {
                'cpu': current['system']['cpu_percent'],
                'memory': current['system']['memory_percent'],
                'disk': current['system']['disk_percent'],
                'response_time': current['requests']['avg_response_time_ms'],
                'error_rate': current['requests']['error_rate_percent']
            }
        }

# Initialize global monitor
monitor = RealTimeMonitor()
monitor.start_monitoring()

# Flask middleware to record requests
@app.before_request
def before_request_monitoring():
    """Record request start time"""
    g.start_time = time.time()

@app.after_request
def after_request_monitoring(response):
    """Record request completion"""
    if hasattr(g, 'start_time'):
        response_time_ms = (time.time() - g.start_time) * 1000
        is_error = response.status_code >= 400
        monitor.record_request(response_time_ms, is_error)
    return response

# Monitoring endpoints
@app.route('/monitoring/dashboard')
@login_required
def monitoring_dashboard():
    """Monitoring dashboard page (admin only)"""
    if not current_user.is_admin:
        abort(403)
    
    return render_template(
        'monitoring/dashboard.html',
        dashboard_data=monitor.get_dashboard_data()
    )

@app.route('/api/monitoring/metrics')
@login_required
def api_monitoring_metrics():
    """API endpoint for live metrics (admin only)"""
    if not current_user.is_admin:
        return jsonify({'error': 'Unauthorized'}), 403
    
    time_range = request.args.get('range', default=5, type=int)
    return jsonify(monitor.get_dashboard_data(time_range))

@app.route('/health')
def health_check():
    """Public health check endpoint"""
    health = monitor.get_health_status()
    status_code = 200 if health['status'] == 'healthy' else 503
    return jsonify(health), status_code
```

### Monitoring Dashboard Template

**File:** `templates/monitoring/dashboard.html` (NEW)

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Monitoring Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #0f1419;
            color: #e6edf3;
            padding: 20px;
        }
        .dashboard { max-width: 1400px; margin: 0 auto; }
        h1 { margin-bottom: 30px; color: #58a6ff; }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }
        .metric-card {
            background: #161b22;
            border: 1px solid #30363d;
            border-radius: 8px;
            padding: 20px;
        }
        .metric-label {
            font-size: 14px;
            color: #8b949e;
            margin-bottom: 8px;
        }
        .metric-value {
            font-size: 32px;
            font-weight: bold;
            color: #58a6ff;
        }
        .metric-value.warning { color: #f0883e; }
        .metric-value.critical { color: #f85149; }
        .metric-threshold {
            font-size: 12px;
            color: #6e7681;
            margin-top: 4px;
        }
        .chart-container {
            background: #161b22;
            border: 1px solid #30363d;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }
        .chart-title {
            font-size: 18px;
            margin-bottom: 15px;
            color: #e6edf3;
        }
        .status-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 600;
        }
        .status-healthy { background: #238636; color: #fff; }
        .status-warning { background: #9e6a03; color: #fff; }
        .status-critical { background: #da3633; color: #fff; }
    </style>
</head>
<body>
    <div class="dashboard">
        <h1>üéØ System Monitoring Dashboard</h1>
        
        <!-- Current Metrics -->
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-label">CPU Usage</div>
                <div class="metric-value" id="cpu-value">--</div>
                <div class="metric-threshold">Threshold: 80%</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-label">Memory Usage</div>
                <div class="metric-value" id="memory-value">--</div>
                <div class="metric-threshold">Threshold: 85%</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-label">Disk Usage</div>
                <div class="metric-value" id="disk-value">--</div>
                <div class="metric-threshold">Threshold: 90%</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-label">Avg Response Time</div>
                <div class="metric-value" id="response-value">--</div>
                <div class="metric-threshold">Threshold: 2000ms</div>
            </div>
            
            <div class="metric-card">
                <div class="metric-label">Error Rate</div>
                <div class="metric-value" id="error-value">--</div>
                <div class="metric-threshold">Threshold: 5%</div>
            </div>
        </div>
        
        <!-- Charts -->
        <div class="chart-container">
            <div class="chart-title">System Resources (Last 5 Minutes)</div>
            <canvas id="systemChart" height="80"></canvas>
        </div>
        
        <div class="chart-container">
            <div class="chart-title">Response Times (Last 5 Minutes)</div>
            <canvas id="responseChart" height="80"></canvas>
        </div>
    </div>
    
    <script>
        // Chart configurations
        const systemCtx = document.getElementById('systemChart').getContext('2d');
        const responseCtx = document.getElementById('responseChart').getContext('2d');
        
        const systemChart = new Chart(systemCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [
                    {
                        label: 'CPU %',
                        data: [],
                        borderColor: '#58a6ff',
                        tension: 0.4
                    },
                    {
                        label: 'Memory %',
                        data: [],
                        borderColor: '#f0883e',
                        tension: 0.4
                    }
                ]
            },
            options: {
                responsive: true,
                scales: {
                    y: { min: 0, max: 100 }
                }
            }
        });
        
        const responseChart = new Chart(responseCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [{
                    label: 'Response Time (ms)',
                    data: [],
                    borderColor: '#a371f7',
                    tension: 0.4
                }]
            },
            options: {
                responsive: true,
                scales: {
                    y: { min: 0 }
                }
            }
        });
        
        // Update dashboard
        async function updateDashboard() {
            try {
                const response = await fetch('/api/monitoring/metrics?range=5');
                const data = await response.json();
                
                if (data.status !== 'ok') return;
                
                // Update current metrics
                updateMetric('cpu', data.current.system.cpu_percent, 80);
                updateMetric('memory', data.current.system.memory_percent, 85);
                updateMetric('disk', data.current.system.disk_percent, 90);
                updateMetric('response', data.current.requests.avg_response_time_ms, 2000);
                updateMetric('error', data.current.requests.error_rate_percent, 5);
                
                // Update charts
                systemChart.data.labels = data.timeline.map(t => t.time);
                systemChart.data.datasets[0].data = data.timeline.map(t => t.cpu);
                systemChart.data.datasets[1].data = data.timeline.map(t => t.memory);
                systemChart.update();
                
                responseChart.data.labels = data.timeline.map(t => t.time);
                responseChart.data.datasets[0].data = data.timeline.map(t => t.response_time);
                responseChart.update();
                
            } catch (error) {
                console.error('Failed to update dashboard:', error);
            }
        }
        
        function updateMetric(id, value, threshold) {
            const element = document.getElementById(`${id}-value`);
            const formatted = id === 'response' ? `${Math.round(value)}ms` : `${value.toFixed(1)}%`;
            element.textContent = formatted;
            
            // Update color based on threshold
            element.classList.remove('warning', 'critical');
            if (value > threshold) {
                element.classList.add('critical');
            } else if (value > threshold * 0.8) {
                element.classList.add('warning');
            }
        }
        
        // Initial load and auto-refresh every 5 seconds
        updateDashboard();
        setInterval(updateDashboard, 5000);
    </script>
</body>
</html>
```

---

## üß™ FEATURE 5: Automated Load Testing Suite

**File:** `tests/load_test.py` (NEW)

```python
"""
Automated Load Testing Suite
Tests system under realistic load conditions
"""

import asyncio
import aiohttp
import time
import statistics
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor
import pytest
import logging

logger = logging.getLogger('load_test')

class LoadTester:
    """
    Comprehensive load testing framework
    
    Features:
    - Concurrent user simulation
    - Multiple endpoint testing
    - Performance metrics collection
    - Automated assertions
    - Detailed reporting
    """
    
    def __init__(self, base_url='http://localhost:5173'):
        """
        Initialize load tester
        
        Args:
            base_url: Base URL of application to test
        """
        self.base_url = base_url
        self.results = []
    
    async def test_pagination_load(
        self, 
        concurrent_users=50, 
        total_requests=1000,
        ramp_up_seconds=10
    ) -> Dict[str, Any]:
        """
        Test pagination endpoints under load
        
        Simulates realistic user behavior:
        - Random page access
        - Natural delays between requests
        - Varied request patterns
        
        Args:
            concurrent_users: Number of concurrent users
            total_requests: Total requests to make
            ramp_up_seconds: Time to ramp up to full load
        
        Returns:
            Dict with comprehensive test results
        """
        logger.info(f"üöÄ Starting load test: {concurrent_users} users, {total_requests} requests")
        
        async def user_session(user_id: int):
            """Simulate single user session"""
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                success_count = 0
                error_count = 0
                response_times = []
                errors_by_code = {}
                
                requests_per_user = total_requests // concurrent_users
                
                # Ramp-up delay
                if ramp_up_seconds > 0:
                    delay = (user_id / concurrent_users) * ramp_up_seconds
                    await asyncio.sleep(delay)
                
                for i in range(requests_per_user):
                    # Random page (simulate realistic browsing)
                    page = (i % 10) + 1  # Pages 1-10
                    
                    request_start = time.time()
                    try:
                        async with session.get(
                            f"{self.base_url}/api/svg/list?page={page}",
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as resp:
                            response_time = (time.time() - request_start) * 1000
                            response_times.append(response_time)
                            
                            if resp.status == 200:
                                success_count += 1
                                
                                # Validate response
                                try:
                                    data = await resp.json()
                                    assert 'items' in data, "Missing 'items' in response"
                                    assert len(data['items']) <= 50, "Too many items returned"
                                except Exception as e:
                                    logger.error(f"Response validation failed: {e}")
                                    error_count += 1
                            else:
                                error_count += 1
                                errors_by_code[resp.status] = errors_by_code.get(resp.status, 0) + 1
                                logger.warning(f"User {user_id}: HTTP {resp.status} on request {i}")
                    
                    except asyncio.TimeoutError:
                        error_count += 1
                        errors_by_code['timeout'] = errors_by_code.get('timeout', 0) + 1
                        logger.error(f"User {user_id}: Request {i} timed out")
                    
                    except Exception as e:
                        error_count += 1
                        errors_by_code['exception'] = errors_by_code.get('exception', 0) + 1
                        logger.error(f"User {user_id}: Request {i} failed: {e}")
                    
                    # Natural delay between requests (100-300ms)
                    await asyncio.sleep(0.1 + (hash(f"{user_id}{i}") % 200) / 1000)
                
                total_time = time.time() - start_time
                
                return {
                    'user_id': user_id,
                    'total_time': total_time,
                    'success_count': success_count,
                    'error_count': error_count,
                    'errors_by_code': errors_by_code,
                    'response_times': response_times,
                    'avg_response_time': statistics.mean(response_times) if response_times else 0,
                    'min_response_time': min(response_times) if response_times else 0,
                    'max_response_time': max(response_times) if response_times else 0,
                    'requests_per_second': success_count / total_time if total_time > 0 else 0
                }
        
        # Run concurrent user sessions
        test_start = time.time()
        tasks = [user_session(i) for i in range(concurrent_users)]
        results = await asyncio.gather(*tasks)
        test_duration = time.time() - test_start
        
        # Aggregate results
        total_requests_made = sum(r['success_count'] + r['error_count'] for r in results)
        total_successful = sum(r['success_count'] for r in results)
        total_errors = sum(r['error_count'] for r in results)
        
        # Collect all response times
        all_response_times = []
        for result in results:
            all_response_times.extend(result['response_times'])
        
        # Aggregate errors by code
        all_errors = {}
        for result in results:
            for code, count in result['errors_by_code'].items():
                all_errors[code] = all_errors.get(code, 0) + count
        
        # Calculate percentiles
        if all_response_times:
            sorted_times = sorted(all_response_times)
            p50 = sorted_times[len(sorted_times) // 2]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
        else:
            p50 = p95 = p99 = 0
        
        return {
            'test_config': {
                'concurrent_users': concurrent_users,
                'total_requests': total_requests,
                'ramp_up_seconds': ramp_up_seconds
            },
            'duration': {
                'total_seconds': test_duration,
                'requests_per_second': total_requests_made / test_duration
            },
            'requests': {
                'total': total_requests_made,
                'successful': total_successful,
                'failed': total_errors,
                'success_rate': (total_successful / total_requests_made * 100) if total_requests_made > 0 else 0
            },
            'response_times': {
                'mean_ms': statistics.mean(all_response_times) if all_response_times else 0,
                'median_ms': p50,
                'min_ms': min(all_response_times) if all_response_times else 0,
                'max_ms': max(all_response_times) if all_response_times else 0,
                'p95_ms': p95,
                'p99_ms': p99,
                'std_dev_ms': statistics.stdev(all_response_times) if len(all_response_times) > 1 else 0
            },
            'errors': all_errors,
            'per_user_stats': {
                'avg_requests_per_second': statistics.mean(r['requests_per_second'] for r in results),
                'avg_success_rate': statistics.mean(
                    r['success_count'] / (r['success_count'] + r['error_count']) * 100
                    for r in results if (r['success_count'] + r['error_count']) > 0
                )
            }
        }
    
    async def test_concurrent_endpoints(self, duration_seconds=60) -> Dict[str, Any]:
        """
        Test multiple endpoints concurrently
        
        Simulates realistic mixed workload:
        - List pages
        - Individual SVG details
        - Likes previews
        
        Args:
            duration_seconds: Test duration
        
        Returns:
            Dict with results by endpoint
        """
        logger.info(f"üîÑ Starting mixed workload test: {duration_seconds}s")
        
        endpoints = [
            '/api/svg/list?page=1',
            '/api/svg/list?page=2',
            '/api/svg/1/likes/preview',
            '/api/svg/2/likes/preview',
        ]
        
        results_by_endpoint = {endpoint: [] for endpoint in endpoints}
        
        async def hit_endpoint(session, endpoint):
            """Hit endpoint and record result"""
            start = time.time()
            try:
                async with session.get(
                    f"{self.base_url}{endpoint}",
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as resp:
                    duration = (time.time() - start) * 1000
                    results_by_endpoint[endpoint].append({
                        'status': resp.status,
                        'duration_ms': duration,
                        'success': resp.status == 200
                    })
            except Exception as e:
                duration = (time.time() - start) * 1000
                results_by_endpoint[endpoint].append({
                    'status': 0,
                    'duration_ms': duration,
                    'success': False,
                    'error': str(e)
                })
        
        async def worker():
            """Worker that continuously hits endpoints"""
            async with aiohttp.ClientSession() as session:
                end_time = time.time() + duration_seconds
                
                while time.time() < end_time:
                    # Pick random endpoint
                    endpoint = endpoints[int(time.time() * 1000) % len(endpoints)]
                    await hit_endpoint(session, endpoint)
                    await asyncio.sleep(0.1)  # Small delay
        
        # Run workers
        workers = [worker() for _ in range(10)]  # 10 concurrent workers
        await asyncio.gather(*workers)
        
        # Analyze results
        summary = {}
        for endpoint, results in results_by_endpoint.items():
            if results:
                durations = [r['duration_ms'] for r in results]
                successes = sum(1 for r in results if r['success'])
                
                summary[endpoint] = {
                    'total_requests': len(results),
                    'successful': successes,
                    'failed': len(results) - successes,
                    'success_rate': (successes / len(results) * 100),
                    'avg_response_time_ms': statistics.mean(durations),
                    'p95_response_time_ms': statistics.quantiles(durations, n=20)[18] if len(durations) > 20 else 0
                }
        
        return summary

# Pytest integration
@pytest.mark.asyncio
@pytest.mark.load
async def test_pagination_performance():
    """Test pagination under load"""
    tester = LoadTester()
    results = await tester.test_pagination_load(
        concurrent_users=50,
        total_requests=1000,
        ramp_up_seconds=10
    )
    
    logger.info(f"üìä Load test results:\n{format_results(results)}")
    
    # Assertions
    assert results['requests']['success_rate'] > 99, \
        f"Success rate too low: {results['requests']['success_rate']:.2f}%"
    
    assert results['response_times']['p95_ms'] < 1000, \
        f"P95 response time too high: {results['response_times']['p95_ms']:.0f}ms"
    
    assert results['response_times']['mean_ms'] < 500, \
        f"Mean response time too high: {results['response_times']['mean_ms']:.0f}ms"
    
    assert results['duration']['requests_per_second'] > 50, \
        f"Throughput too low: {results['duration']['requests_per_second']:.1f} req/s"

@pytest.mark.asyncio
@pytest.mark.load
async def test_mixed_workload():
    """Test mixed endpoint workload"""
    tester = LoadTester()
    results = await tester.test_concurrent_endpoints(duration_seconds=30)
    
    # All endpoints should perform well
    for endpoint, stats in results.items():
        assert stats['success_rate'] > 95, \
            f"{endpoint}: Success rate too low: {stats['success_rate']:.1f}%"
        
        assert stats['avg_response_time_ms'] < 1000, \
            f"{endpoint}: Avg response time too high: {stats['avg_response_time_ms']:.0f}ms"

def format_results(results: Dict[str, Any]) -> str:
    """Format results for readable output"""
    return f"""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë        LOAD TEST RESULTS                  ‚ïë
    ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
    ‚ïë Duration: {results['duration']['total_seconds']:.1f}s
    ‚ïë Throughput: {results['duration']['requests_per_second']:.1f} req/s
    ‚ïë
    ‚ïë Requests:
    ‚ïë   Total: {results['requests']['total']}
    ‚ïë   Successful: {results['requests']['successful']}
    ‚ïë   Failed: {results['requests']['failed']}
    ‚ïë   Success Rate: {results['requests']['success_rate']:.2f}%
    ‚ïë
    ‚ïë Response Times:
    ‚ïë   Mean: {results['response_times']['mean_ms']:.0f}ms
    ‚ïë   Median: {results['response_times']['median_ms']:.0f}ms
    ‚ïë   P95: {results['response_times']['p95_ms']:.0f}ms
    ‚ïë   P99: {results['response_times']['p99_ms']:.0f}ms
    ‚ïë   Min: {results['response_times']['min_ms']:.0f}ms
    ‚ïë   Max: {results['response_times']['max_ms']:.0f}ms
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
```

---

## üì± FEATURE 6: Progressive Web App (PWA) Support

**File:** `static/js/service-worker.js` (NEW)

```javascript
/**
 * Service Worker for Offline Support
 * Implements PWA caching strategies
 */

const CACHE_NAME = 'tikz2svg-v1.0.0';
const RUNTIME_CACHE = 'tikz2svg-runtime-v1';

// Static assets to cache on install
const STATIC_CACHE_URLS = [
    '/',
    '/static/css/foundation/master-variables.css',
    '/static/css/foundation/global-base.css',
    '/static/css/index.css',
    '/static/js/lazy-loading-utils.js',
    '/static/js/file_card.js',
    '/static/js/error-handler.js',
    '/offline.html'
];

/**
 * Install event - cache static assets
 */
self.addEventListener('install', event => {
    console.log('üîß Service Worker installing...');
    
    event.waitUntil(
        caches.open(CACHE_NAME)
            .then(cache => {
                console.log('üì¶ Caching static assets');
                return cache.addAll(STATIC_CACHE_URLS);
            })
            .then(() => self.skipWaiting())
    );
});

/**
 * Activate event - clean up old caches
 */
self.addEventListener('activate', event => {
    console.log('‚úÖ Service Worker activated');
    
    event.waitUntil(
        caches.keys().then(cacheNames => {
            return Promise.all(
                cacheNames.map(cacheName => {
                    if (cacheName !== CACHE_NAME && cacheName !== RUNTIME_CACHE) {
                        console.log('üóëÔ∏è Deleting old cache:', cacheName);
                        return caches.delete(cacheName);
                    }
                })
            );
        }).then(() => self.clients.claim())
    );
});

/**
 * Fetch event - implement caching strategies
 */
self.addEventListener('fetch', event => {
    const { request } = event;
    const url = new URL(request.url);
    
    // Skip non-GET requests
    if (request.method !== 'GET') {
        return;
    }
    
    // API requests: Network first, cache fallback
    if (url.pathname.startsWith('/api/')) {
        event.respondWith(networkFirst(request));
        return;
    }
    
    // Static assets: Cache first, network fallback
    if (isStaticAsset(url.pathname)) {
        event.respondWith(cacheFirst(request));
        return;
    }
    
    // HTML pages: Network first, cache fallback
    if (request.headers.get('accept').includes('text/html')) {
        event.respondWith(networkFirst(request));
        return;
    }
    
    // Default: Network first
    event.respondWith(networkFirst(request));
});

/**
 * Cache-first strategy
 * Good for static assets that rarely change
 */
async function cacheFirst(request) {
    const cache = await caches.open(CACHE_NAME);
    const cached = await cache.match(request);
    
    if (cached) {
        console.log('üéØ Cache hit:', request.url);
        return cached;
    }
    
    try {
        const response = await fetch(request);
        
        // Cache successful responses
        if (response.ok) {
            cache.put(request, response.clone());
        }
        
        return response;
    } catch (error) {
        console.error('‚ùå Fetch failed:', error);
        
        // Return offline page for navigation requests
        if (request.mode === 'navigate') {
            return caches.match('/offline.html');
        }
        
        throw error;
    }
}

/**
 * Network-first strategy
 * Good for dynamic content that needs to be fresh
 */
async function networkFirst(request) {
    const cache = await caches.open(RUNTIME_CACHE);
    
    try {
        const response = await fetch(request);
        
        // Cache successful responses
        if (response.ok) {
            cache.put(request, response.clone());
        }
        
        return response;
    } catch (error) {
        console.error('‚ùå Network request failed:', error);
        
        // Try cache fallback
        const cached = await cache.match(request);
        if (cached) {
            console.log('üì¶ Using cached version:', request.url);
            return cached;
        }
        
        // Return offline page for navigation requests
        if (request.mode === 'navigate') {
            return caches.match('/offline.html');
        }
        
        throw error;
    }
}

/**
 * Check if URL is a static asset
 */
function isStaticAsset(pathname) {
    return pathname.startsWith('/static/') ||
           pathname.match(/\.(css|js|png|jpg|jpeg|svg|gif|woff|woff2|ttf|eot)$/);
}

/**
 * Background sync for failed requests
 */
self.addEventListener('sync', event => {
    if (event.tag === 'sync-likes') {
        event.waitUntil(syncLikes());
    }
});

async function syncLikes() {
    // Retry failed like/unlike requests
    console.log('üîÑ Syncing likes...');
    // Implementation depends on your needs
}
```

**File:** `static/js/pwa-init.js` (NEW)

```javascript
/**
 * PWA Initialization
 * Register service worker and handle install prompt
 */

// Register service worker
if ('serviceWorker' in navigator) {
    window.addEventListener('load', () => {
        navigator.serviceWorker.register('/static/js/service-worker.js')
            .then(registration => {
                console.log('‚úÖ Service Worker registered:', registration.scope);
                
                // Check for updates
                registration.addEventListener('updatefound', () => {
                    const newWorker = registration.installing;
                    
                    newWorker.addEventListener('statechange', () => {
                        if (newWorker.state === 'installed' && navigator.serviceWorker.controller) {
                            // New version available
                            showUpdateNotification();
                        }
                    });
                });
            })
            .catch(error => {
                console.error('‚ùå Service Worker registration failed:', error);
            });
    });
}

// Handle install prompt
let deferredPrompt;

window.addEventListener('beforeinstallprompt', (e) => {
    // Prevent default prompt
    e.preventDefault();
    
    // Store event for later
    deferredPrompt = e;
    
    // Show custom install button
    showInstallButton();
});

function showInstallButton() {
    const installButton = document.getElementById('install-pwa-button');
    if (installButton) {
        installButton.style.display = 'block';
        
        installButton.addEventListener('click', async () => {
            if (!deferredPrompt) return;
            
            // Show install prompt
            deferredPrompt.prompt();
            
            // Wait for user choice
            const { outcome } = await deferredPrompt.userChoice;
            console.log(`User ${outcome} the install prompt`);
            
            // Clear prompt
            deferredPrompt = null;
            installButton.style.display = 'none';
        });
    }
}

function showUpdateNotification() {
    // Show user that update is available
    const notification = document.createElement('div');
    notification.className = 'update-notification';
    notification.innerHTML = `
        <div class="update-content">
            <p>üéâ A new version is available!</p>
            <button onclick="window.location.reload()">Refresh</button>
        </div>
    `;
    document.body.appendChild(notification);
}
```

**File:** `static/manifest.json` (NEW)

```json
{
  "name": "TikZ2SVG Converter",
  "short_name": "TikZ2SVG",
  "description": "Convert TikZ code to SVG images",
  "start_url": "/",
  "display": "standalone",
  "background_color": "#0f1419",
  "theme_color": "#58a6ff",
  "orientation": "portrait-primary",
  "icons": [
    {
      "src": "/static/images/icon-192.png",
      "sizes": "192x192",
      "type": "image/png",
      "purpose": "any maskable"
    },
    {
      "src": "/static/images/icon-512.png",
      "sizes": "512x512",
      "type": "image/png",
      "purpose": "any maskable"
    }
  ],
  "categories": ["productivity", "utilities"],
  "screenshots": [
    {
      "src": "/static/images/screenshot-1.png",
      "sizes": "1280x720",
      "type": "image/png"
    }
  ]
}
```

**File:** `templates/offline.html` (NEW)

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Offline - TikZ2SVG</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            color: white;
            text-align: center;
            padding: 20px;
        }
        .offline-container {
            max-width: 500px;
        }
        .offline-icon {
            font-size: 120px;
            margin-bottom: 30px;
            animation: bounce 2s infinite;
        }
        @keyframes bounce {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-20px); }
        }
        h1 {
            font-size: 36px;
            margin-bottom: 20px;
        }
        p {
            font-size: 18px;
            line-height: 1.6;
            opacity: 0.9;
            margin-bottom: 30px;
        }
        .retry-button {
            background: white;
            color: #667eea;
            border: none;
            padding: 15px 40px;
            font-size: 16px;
            font-weight: 600;
            border-radius: 8px;
            cursor: pointer;
            transition: transform 0.2s;
        }
        .retry-button:hover {
            transform: scale(1.05);
        }
    </style>
</head>
<body>
    <div class="offline-container">
        <div class="offline-icon">üì°</div>
        <h1>You're Offline</h1>
        <p>
            It looks like you've lost your internet connection. 
            Don't worry, you can still browse cached pages!
        </p>
        <button class="retry-button" onclick="window.location.reload()">
            Try Again
        </button>
    </div>
</body>
</html>
```

---

## üéØ Integration Summary

### Files Created (Part 2):
1. `monitoring/real_time_monitor.py` - Real-time monitoring system
2. `templates/monitoring/dashboard.html` - Monitoring dashboard UI
3. `tests/load_test.py` - Automated load testing suite
4. `static/js/service-worker.js` - PWA service worker
5. `static/js/pwa-init.js` - PWA initialization
6. `static/manifest.json` - PWA manifest
7. `templates/offline.html` - Offline fallback page

### Next Steps:
1. **Add PWA meta tags to `base.html`**:
```html
<link rel="manifest" href="/static/manifest.json">
<meta name="theme-color" content="#58a6ff">
```

2. **Run load tests**:
```bash
pytest tests/load_test.py -v --log-cli-level=INFO
```

3. **Access monitoring dashboard**:
```
http://localhost:5173/monitoring/dashboard
```

4. **Test PWA offline capability**:
- Open DevTools ‚Üí Application ‚Üí Service Workers
- Check "Offline" mode
- Navigate site

---

## üèÜ Complete Enterprise Feature Set

**Part 1 + Part 2 provides:**
‚úÖ Advanced Security (IP fingerprinting, CSP)  
‚úÖ Database Connection Pooling (thread-safe)  
‚úÖ Multi-Level Caching (L1 + L2)  
‚úÖ Real-Time Monitoring (live metrics)  
‚úÖ Load Testing Suite (automated validation)  
‚úÖ PWA Support (offline capability)

**Total Implementation Time:** ~6-8 hours  
**Production Readiness:** üåüüåüüåüüåüüåü (5/5)

B·∫°n c√≥ mu·ªën t√¥i t·∫°o **Installation Guide** ƒë·ªÉ implement t·∫•t c·∫£ features n√†y kh√¥ng? üöÄ

